{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nfrom scipy import io as sio\nimport sklearn.metrics as sm\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nimport json\n\n# TCN Model Implementations (inspired by Lea et al. research)\n\nclass TemporalConv1D(nn.Module):\n    \"\"\"Basic temporal convolution with causal padding\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, causal=True):\n        super(TemporalConv1D, self).__init__()\n        self.causal = causal\n        if causal:\n            # Causal padding: pad only on the left side\n            self.padding = (kernel_size - 1) * dilation\n        else:\n            # Acausal padding: pad on both sides\n            self.padding = (kernel_size - 1) * dilation // 2\n            \n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n                             stride=stride, dilation=dilation, padding=0)\n        \n    def forward(self, x):\n        if self.causal:\n            x = F.pad(x, (self.padding, 0))\n        else:\n            pad_left = self.padding\n            pad_right = self.padding\n            x = F.pad(x, (pad_left, pad_right))\n        return self.conv(x)\n\nclass DilatedTCN(nn.Module):\n    \"\"\"Dilated TCN based on WaveNet architecture (Lea et al.)\"\"\"\n    def __init__(self, n_feat, n_classes, n_channels=64, n_layers=8, kernel_size=3, \n                 dropout=0.2, causal=True, activation='relu'):\n        super(DilatedTCN, self).__init__()\n        \n        self.n_layers = n_layers\n        self.causal = causal\n        \n        # Input projection\n        self.input_conv = nn.Conv1d(n_feat, n_channels, 1)\n        \n        # Dilated convolution layers\n        self.dilated_convs = nn.ModuleList()\n        self.residual_convs = nn.ModuleList()\n        self.skip_convs = nn.ModuleList()\n        self.bn_layers = nn.ModuleList()\n        \n        for i in range(n_layers):\n            dilation = 2 ** i\n            \n            # Dilated convolution\n            dilated_conv = TemporalConv1D(n_channels, n_channels, kernel_size, \n                                        dilation=dilation, causal=causal)\n            self.dilated_convs.append(dilated_conv)\n            \n            # Residual connection\n            self.residual_convs.append(nn.Conv1d(n_channels, n_channels, 1))\n            \n            # Skip connection\n            self.skip_convs.append(nn.Conv1d(n_channels, n_channels, 1))\n            \n            # Batch normalization\n            self.bn_layers.append(nn.BatchNorm1d(n_channels))\n        \n        # Output layers\n        self.output_conv1 = nn.Conv1d(n_channels, n_channels, 1)\n        self.output_conv2 = nn.Conv1d(n_channels, n_classes, 1)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Activation function\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        else:\n            self.activation = nn.ReLU()\n    \n    def forward(self, x):\n        # Input: (batch_size, sequence_length, n_features)\n        # Convert to (batch_size, n_features, sequence_length)\n        x = x.transpose(1, 2)\n        \n        # Input projection\n        x = self.input_conv(x)\n        \n        skip_connections = []\n        \n        for i in range(self.n_layers):\n            # Store input for residual connection\n            residual = x\n            \n            # Dilated convolution\n            x = self.dilated_convs[i](x)\n            x = self.bn_layers[i](x)\n            x = self.activation(x)\n            x = self.dropout(x)\n            \n            # Skip connection\n            skip = self.skip_convs[i](x)\n            skip_connections.append(skip)\n            \n            # Residual connection\n            x = self.residual_convs[i](x) + residual\n        \n        # Combine skip connections\n        x = sum(skip_connections)\n        x = self.activation(x)\n        \n        # Output layers\n        x = self.output_conv1(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        x = self.output_conv2(x)\n        \n        # Convert back to (batch_size, sequence_length, n_classes)\n        x = x.transpose(1, 2)\n        \n        return x\n\nclass ED_TCN(nn.Module):\n    \"\"\"Encoder-Decoder TCN (Lea et al.)\"\"\"\n    def __init__(self, n_nodes, kernel_size, n_classes, n_feat, causal=True, \n                 activation='relu', dropout=0.2):\n        super(ED_TCN, self).__init__()\n        \n        self.n_layers = len(n_nodes)\n        self.causal = causal\n        \n        # Encoder\n        self.encoder_convs = nn.ModuleList()\n        self.encoder_bn = nn.ModuleList()\n        \n        in_channels = n_feat\n        for i, out_channels in enumerate(n_nodes):\n            conv = TemporalConv1D(in_channels, out_channels, kernel_size, causal=causal)\n            self.encoder_convs.append(conv)\n            self.encoder_bn.append(nn.BatchNorm1d(out_channels))\n            in_channels = out_channels\n        \n        # Decoder\n        self.decoder_convs = nn.ModuleList()\n        self.decoder_bn = nn.ModuleList()\n        \n        for i in range(self.n_layers):\n            if i == 0:\n                in_channels = n_nodes[-1]\n                out_channels = n_nodes[-1-i]\n            else:\n                in_channels = n_nodes[-i] * 2  # Skip connections double the channels\n                out_channels = n_nodes[-1-i]\n            \n            conv = TemporalConv1D(in_channels, out_channels, kernel_size, causal=causal)\n            self.decoder_convs.append(conv)\n            self.decoder_bn.append(nn.BatchNorm1d(out_channels))\n        \n        # Output layer\n        self.output_conv = nn.Conv1d(n_nodes[0], n_classes, 1)\n        \n        # Activation and dropout\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        else:\n            self.activation = nn.ReLU()\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        # Input: (batch_size, sequence_length, n_features)\n        # Convert to (batch_size, n_features, sequence_length)\n        x = x.transpose(1, 2)\n        \n        # Encoder with skip connections\n        encoder_outputs = []\n        for i in range(self.n_layers):\n            x = self.encoder_convs[i](x)\n            x = self.encoder_bn[i](x)\n            x = self.activation(x)\n            x = self.dropout(x)\n            encoder_outputs.append(x)\n        \n        # Decoder with skip connections\n        for i in range(self.n_layers):\n            if i > 0:\n                # Add skip connection from encoder\n                skip_idx = self.n_layers - 1 - i\n                x = torch.cat([x, encoder_outputs[skip_idx]], dim=1)\n            \n            x = self.decoder_convs[i](x)\n            x = self.decoder_bn[i](x)\n            x = self.activation(x)\n            x = self.dropout(x)\n        \n        # Output layer\n        x = self.output_conv(x)\n        \n        # Convert back to (batch_size, sequence_length, n_classes)\n        x = x.transpose(1, 2)\n        \n        return x\n\nclass tCNN(nn.Module):\n    \"\"\"Temporal CNN (Lea et al. ECCV 2016)\"\"\"\n    def __init__(self, n_nodes, kernel_size, n_classes, n_feat, causal=True, dropout=0.2):\n        super(tCNN, self).__init__()\n        \n        self.convs = nn.ModuleList()\n        self.bn_layers = nn.ModuleList()\n        \n        in_channels = n_feat\n        for out_channels in n_nodes:\n            conv = TemporalConv1D(in_channels, out_channels, kernel_size, causal=causal)\n            self.convs.append(conv)\n            self.bn_layers.append(nn.BatchNorm1d(out_channels))\n            in_channels = out_channels\n        \n        # Output layer\n        self.output_conv = nn.Conv1d(n_nodes[-1], n_classes, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = nn.ReLU()\n    \n    def forward(self, x):\n        # Input: (batch_size, sequence_length, n_features)\n        # Convert to (batch_size, n_features, sequence_length)\n        x = x.transpose(1, 2)\n        \n        for i, (conv, bn) in enumerate(zip(self.convs, self.bn_layers)):\n            x = conv(x)\n            x = bn(x)\n            x = self.activation(x)\n            x = self.dropout(x)\n        \n        # Output layer\n        x = self.output_conv(x)\n        \n        # Convert back to (batch_size, sequence_length, n_classes)\n        x = x.transpose(1, 2)\n        \n        return x\n\n# ============================================================================\n# Video Feature Extraction\n# ============================================================================\n\nclass VideoFeatureExtractor(nn.Module):\n    \"\"\"Extract spatial features from video frames\"\"\"\n    def __init__(self, feature_dim=512):\n        super(VideoFeatureExtractor, self).__init__()\n        \n        # Spatial CNN for frame-level features\n        self.spatial_cnn = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            \n            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            \n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        \n        self.fc = nn.Linear(512, feature_dim)\n        \n    def forward(self, x):\n        # x shape: (batch_size, sequence_length, channels, height, width)\n        batch_size, seq_len, c, h, w = x.shape\n        \n        # Process each frame\n        x = x.view(-1, c, h, w)\n        features = self.spatial_cnn(x)\n        features = features.view(features.size(0), -1)\n        features = self.fc(features)\n        \n        # Reshape back to sequence format\n        features = features.view(batch_size, seq_len, -1)\n        return features\n\n# ============================================================================\n# Complete Model\n# ============================================================================\n\nclass TCNEmotionClassifier(nn.Module):\n    \"\"\"Complete TCN-based emotion classifier\"\"\"\n    def __init__(self, model_type=\"ED-TCN\", n_feat=512, n_classes=6, n_nodes=[64, 96], \n                 kernel_size=3, causal=True, dropout=0.2, max_len=None):\n        super(TCNEmotionClassifier, self).__init__()\n        \n        self.model_type = model_type\n        self.n_classes = n_classes\n        self.max_len = max_len\n        \n        # Feature extractor\n        self.feature_extractor = VideoFeatureExtractor(n_feat)\n        \n        # Temporal model\n        if model_type == \"DilatedTCN\":\n            self.temporal_model = DilatedTCN(n_feat, n_classes, n_channels=n_nodes[0], \n                                           n_layers=8, kernel_size=kernel_size, \n                                           dropout=dropout, causal=causal)\n        elif model_type == \"ED-TCN\":\n            self.temporal_model = ED_TCN(n_nodes, kernel_size, n_classes, n_feat, \n                                       causal=causal, dropout=dropout)\n        elif model_type == \"tCNN\":\n            self.temporal_model = tCNN(n_nodes, kernel_size, n_classes, n_feat, \n                                     causal=causal, dropout=dropout)\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n    \n    def forward(self, x, mask=None):\n        # Extract spatial features\n        features = self.feature_extractor(x)\n        \n        # Apply temporal model\n        output = self.temporal_model(features)\n        \n        # Apply mask if provided (for variable length sequences)\n        if mask is not None:\n            output = output * mask.unsqueeze(-1)\n        \n        # For multi-label classification, apply sigmoid\n        return torch.sigmoid(output)\n\n# ============================================================================\n# Dataset and Training\n# ============================================================================\n\nclass VideoEmotionDataset(Dataset):\n    \"\"\"Dataset for video emotion recognition\"\"\"\n    def __init__(self, video_paths, labels, sequence_length=32, frame_size=(224, 224)):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.sequence_length = sequence_length\n        self.frame_size = frame_size\n        \n    def __len__(self):\n        return len(self.video_paths)\n    \n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        label = self.labels[idx]\n        \n        # Load video frames\n        frames = self.load_video_frames(video_path)\n        \n        return frames, torch.FloatTensor(label)\n    \n    def load_video_frames(self, video_path):\n        cap = cv2.VideoCapture(video_path)\n        frames = []\n        \n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        # Sample frames uniformly\n        if total_frames > self.sequence_length:\n            frame_indices = np.linspace(0, total_frames - 1, self.sequence_length, dtype=int)\n        else:\n            frame_indices = list(range(total_frames))\n            # Pad with last frame if needed\n            while len(frame_indices) < self.sequence_length:\n                frame_indices.append(frame_indices[-1])\n        \n        for frame_idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            \n            if ret:\n                frame = cv2.resize(frame, self.frame_size)\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = frame.astype(np.float32) / 255.0\n                frames.append(frame)\n            else:\n                # Use zero frame if reading fails\n                frames.append(np.zeros((*self.frame_size, 3), dtype=np.float32))\n        \n        cap.release()\n        \n        # Convert to tensor format\n        frames = np.stack(frames)\n        frames = torch.FloatTensor(frames).permute(0, 3, 1, 2)\n        \n        return frames\n\ndef mask_data(X, Y, max_len, mask_value=-1):\n    \"\"\"Mask sequences to same length for batch processing\"\"\"\n    X_masked = []\n    Y_masked = []\n    masks = []\n    \n    for x, y in zip(X, Y):\n        seq_len = len(x)\n        \n        if seq_len < max_len:\n            # Pad sequences\n            pad_len = max_len - seq_len\n            x_pad = np.full((pad_len, x.shape[1]), mask_value, dtype=x.dtype)\n            y_pad = np.zeros((pad_len, y.shape[1]), dtype=y.dtype)\n            \n            x_masked = np.vstack([x, x_pad])\n            y_masked = np.vstack([y, y_pad])\n            \n            mask = np.concatenate([np.ones(seq_len), np.zeros(pad_len)])\n        else:\n            # Truncate if too long\n            x_masked = x[:max_len]\n            y_masked = y[:max_len]\n            mask = np.ones(max_len)\n        \n        X_masked.append(x_masked)\n        Y_masked.append(y_masked)\n        masks.append(mask)\n    \n    return np.array(X_masked), np.array(Y_masked), np.array(masks)\n\ndef unmask_predictions(predictions, masks):\n    \"\"\"Remove padding from predictions\"\"\"\n    unmasked = []\n    for pred, mask in zip(predictions, masks):\n        seq_len = int(mask.sum())\n        unmasked.append(pred[:seq_len])\n    return unmasked\n\n# ============================================================================\n# Training and Evaluation\n# ============================================================================\n\nclass EmotionTrainer:\n    \"\"\"Training pipeline for TCN emotion classifier\"\"\"\n    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.device = device\n        self.emotion_labels = ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n        \n    def train_epoch(self, dataloader, optimizer, criterion, max_len=None):\n        self.model.train()\n        total_loss = 0\n        all_predictions = []\n        all_labels = []\n        \n        for batch_idx, (data, target) in enumerate(dataloader):\n            # Handle variable length sequences\n            if max_len:\n                # Convert to feature sequences first\n                batch_features = []\n                batch_labels = []\n                \n                for i in range(data.size(0)):\n                    frames = data[i].unsqueeze(0)\n                    features = self.model.feature_extractor(frames)\n                    batch_features.append(features.squeeze(0))\n                    batch_labels.append(target[i])\n                \n                # Mask to same length\n                X_masked, Y_masked, masks = mask_data(\n                    [f.cpu().numpy() for f in batch_features],\n                    [l.unsqueeze(0).repeat(f.size(0), 1).cpu().numpy() for f, l in zip(batch_features, batch_labels)],\n                    max_len\n                )\n                \n                data = torch.FloatTensor(X_masked).to(self.device)\n                target = torch.FloatTensor(Y_masked).to(self.device)\n                mask = torch.FloatTensor(masks).to(self.device)\n                \n                # Forward pass with temporal model only\n                output = self.model.temporal_model(data)\n                output = output * mask.unsqueeze(-1)\n                \n                # Compute loss only on valid time steps\n                loss = criterion(output.view(-1, output.size(-1)), \n                               target.view(-1, target.size(-1)))\n                \n                # Weight loss by mask\n                mask_flat = mask.view(-1)\n                loss = (loss.mean(dim=1) * mask_flat).sum() / mask_flat.sum()\n                \n            else:\n                data, target = data.to(self.device), target.to(self.device)\n                output = self.model(data)\n                \n                # For sequence output, take mean over time\n                if len(output.shape) == 3:\n                    output = output.mean(dim=1)\n                    \n                loss = criterion(output, target)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            \n            if batch_idx % 10 == 0:\n                print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')\n        \n        return total_loss / len(dataloader)\n    \n    def evaluate(self, dataloader, criterion, max_len=None):\n        self.model.eval()\n        total_loss = 0\n        all_predictions = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for data, target in dataloader:\n                if max_len:\n                    # Similar masking for evaluation\n                    batch_features = []\n                    batch_labels = []\n                    \n                    for i in range(data.size(0)):\n                        frames = data[i].unsqueeze(0)\n                        features = self.model.feature_extractor(frames)\n                        batch_features.append(features.squeeze(0))\n                        batch_labels.append(target[i])\n                    \n                    X_masked, Y_masked, masks = mask_data(\n                        [f.cpu().numpy() for f in batch_features],\n                        [l.unsqueeze(0).repeat(f.size(0), 1).cpu().numpy() for f, l in zip(batch_features, batch_labels)],\n                        max_len\n                    )\n                    \n                    data = torch.FloatTensor(X_masked).to(self.device)\n                    target = torch.FloatTensor(Y_masked).to(self.device)\n                    mask = torch.FloatTensor(masks).to(self.device)\n                    \n                    output = self.model.temporal_model(data)\n                    output = output * mask.unsqueeze(-1)\n                    \n                    # Unmask predictions for evaluation\n                    pred_unmasked = unmask_predictions(output.cpu().numpy(), masks)\n                    label_unmasked = unmask_predictions(Y_masked, masks)\n                    \n                    for p, l in zip(pred_unmasked, label_unmasked):\n                        all_predictions.append((p.mean(axis=0) > 0.5).astype(int))\n                        all_labels.append(l[0])  # Take first label (they're all the same)\n                        \n                else:\n                    data, target = data.to(self.device), target.to(self.device)\n                    output = self.model(data)\n                    \n                    if len(output.shape) == 3:\n                        output = output.mean(dim=1)\n                    \n                    predictions = (output > 0.5).float()\n                    all_predictions.extend(predictions.cpu().numpy())\n                    all_labels.extend(target.cpu().numpy())\n        \n        # Compute metrics\n        all_predictions = np.array(all_predictions)\n        all_labels = np.array(all_labels)\n        \n        accuracy = sm.accuracy_score(all_labels, all_predictions)\n        f1_macro = sm.f1_score(all_labels, all_predictions, average='macro')\n        f1_micro = sm.f1_score(all_labels, all_predictions, average='micro')\n        \n        return accuracy, f1_macro, f1_micro\n    \n    def train(self, train_loader, val_loader, num_epochs=50, lr=0.001, max_len=None):\n        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n        criterion = nn.BCELoss()\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n        \n        best_f1 = 0\n        train_losses = []\n        val_accuracies = []\n        val_f1_scores = []\n        \n        for epoch in range(num_epochs):\n            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n            print('-' * 50)\n            \n            # Training\n            train_loss = self.train_epoch(train_loader, optimizer, criterion, max_len)\n            train_losses.append(train_loss)\n            \n            # Validation\n            val_acc, val_f1_macro, val_f1_micro = self.evaluate(val_loader, criterion, max_len)\n            val_accuracies.append(val_acc)\n            val_f1_scores.append(val_f1_macro)\n            \n            scheduler.step()\n            \n            print(f'Train Loss: {train_loss:.4f}')\n            print(f'Val Acc: {val_acc:.4f}, Val F1 (macro): {val_f1_macro:.4f}, Val F1 (micro): {val_f1_micro:.4f}')\n            \n            # Save best model\n            if val_f1_macro > best_f1:\n                best_f1 = val_f1_macro\n                torch.save(self.model.state_dict(), f'best_{self.model.model_type}_emotion_model.pth')\n                print(f'New best model saved with Val F1: {val_f1_macro:.4f}')\n        \n        return {\n            'train_losses': train_losses,\n            'val_accuracies': val_accuracies,\n            'val_f1_scores': val_f1_scores\n        }\n\n# ============================================================================\n# Example Usage\n# ============================================================================\n\ndef main():\n    # Model configuration\n    model_type = \"ED-TCN\"  # Options: \"DilatedTCN\", \"ED-TCN\", \"tCNN\"\n    n_nodes = [64, 96, 128]\n    kernel_size = 3\n    causal = True  # Set to False for bidirectional/acausal\n    n_classes = 6  # Number of emotion classes\n    sequence_length = 32\n    max_len = 64  # For masking variable length sequences\n    \n    # Initialize model\n    model = TCNEmotionClassifier(\n        model_type=model_type,\n        n_feat=512,\n        n_classes=n_classes,\n        n_nodes=n_nodes,\n        kernel_size=kernel_size,\n        causal=causal,\n        max_len=max_len\n    )\n    \n    print(f\"Model: {model_type}\")\n    print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")\n    print(f\"Causal: {causal}\")\n    \n    # Example training (replace with your data)\n    # train_paths = [\"path/to/video1.mp4\", ...]\n    # train_labels = [[1, 0, 0, 1, 0, 0], ...]  # Multi-label format\n    # \n    # train_dataset = VideoEmotionDataset(train_paths, train_labels, sequence_length)\n    # train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n    # \n    # trainer = EmotionTrainer(model)\n    # history = trainer.train(train_loader, val_loader, num_epochs=50, max_len=max_len)\n    \n    print(\"Model ready for training!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}